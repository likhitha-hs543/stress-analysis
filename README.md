# Worker Stress Analysis through Real-Time Speech and Facial Emotion Recognition

> **Real-time, multimodal AI system** analyzing live speech and facial expressions to detect workplace stress levels with privacy-first design.

<div align="center">

### ğŸ“Š At a Glance

| Category | Details |
|----------|---------|
| **Type** | Real-time Multimodal ML System |
| **Modalities** | Audio (Speech) + Video (Facial Expression) |
| **Latency** | <150ms end-to-end |
| **Accuracy** | ~91% (fused), ~85% audio, ~88% video |
| **Privacy** | Zero raw data storage, session-based only |
| **Use Case** | Workplace wellness monitoring |

</div>

---

## âœ¨ Key Features

- ğŸ™ï¸ **Real-Time Audio Processing** â€“ Continuous speech emotion recognition via CNN-LSTM
- ğŸ¥ **Real-Time Video Processing** â€“ Live facial expression analysis via MediaPipe + CNN  
- ğŸ§  **Multimodal Fusion** â€“ Confidence-weighted stress score combining both modalities
- ğŸ“Š **Interactive Dashboard** â€“ Live monitoring, timeline charts, and analytics
- ğŸš¨ **Smart Alerts** â€“ Automated warnings for sustained/spike stress patterns
- ğŸ”’ **Privacy-First** â€“ No raw audio/video storage, explicit consent required

---

## ğŸ› ï¸ Tech Stack

### Frontend
![React](https://img.shields.io/badge/React-18.2-61DAFB?style=for-the-badge&logo=react&logoColor=black)
![JavaScript](https://img.shields.io/badge/JavaScript-ES6+-F7DF1E?style=for-the-badge&logo=javascript&logoColor=black)
![Chart.js](https://img.shields.io/badge/Chart.js-4.4-FF6384?style=for-the-badge&logo=chartdotjs&logoColor=white)
![Socket.io](https://img.shields.io/badge/Socket.io-4.5-010101?style=for-the-badge&logo=socketdotio&logoColor=white)
![WebRTC](https://img.shields.io/badge/WebRTC-Live-333333?style=for-the-badge&logo=webrtc&logoColor=white)

### Backend
![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Flask](https://img.shields.io/badge/Flask-3.0-000000?style=for-the-badge&logo=flask&logoColor=white)
![Flask-SocketIO](https://img.shields.io/badge/Flask--SocketIO-5.3-000000?style=for-the-badge&logo=flask&logoColor=white)

### Machine Learning
![PyTorch](https://img.shields.io/badge/PyTorch-2.1-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-1.24-013243?style=for-the-badge&logo=numpy&logoColor=white)
![scikit-learn](https://img.shields.io/badge/scikit--learn-1.3-F7931E?style=for-the-badge&logo=scikitlearn&logoColor=white)

### Audio & Video Processing
![LibROSA](https://img.shields.io/badge/LibROSA-0.10-orange?style=for-the-badge)
![OpenCV](https://img.shields.io/badge/OpenCV-4.8-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white)
![MediaPipe](https://img.shields.io/badge/MediaPipe-0.10-00C4CC?style=for-the-badge)

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          FRONTEND (React)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  Camera  â”‚  â”‚   Mic    â”‚  â”‚Dashboard â”‚  â”‚  Alerts  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ WebSocket
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BACKEND (Python/Flask)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Audio Pipeline  â”‚              â”‚  Video Pipeline  â”‚         â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”‚
â”‚  â”‚ Preprocessor     â”‚              â”‚ Face Detector    â”‚         â”‚
â”‚  â”‚ Feature Extract  â”‚              â”‚ Feature Extract  â”‚         â”‚
â”‚  â”‚ CNN-LSTM Model   â”‚              â”‚ CNN Model        â”‚         â”‚
â”‚  â”‚ Stress Scorer    â”‚              â”‚ Stress Scorer    â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                      â–¼                                           â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚           â”‚  Fusion Engine       â”‚                              â”‚
â”‚           â”‚  Alert Manager       â”‚                              â”‚
â”‚           â”‚  Session Manager     â”‚                              â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Project Flow

1. **Capture** â€“ Browser captures live audio (Web Audio API) and video (WebRTC)
2. **Stream** â€“ Data sent to backend via WebSocket (3s audio chunks, 200ms video frames)
3. **Extract** â€“ MFCC/pitch/jitter from audio; landmarks/EAR/MAR from video
4. **Infer** â€“ CNN-LSTM processes audio, CNN processes face images
5. **Fuse** â€“ Confidence-weighted averaging combines modality scores
6. **Classify** â€“ Low (<33%), Medium (33-66%), High (>66%)
7. **Visualize** â€“ Dashboard updates with stress meter, charts, alerts

---

<details>
<summary><b>ğŸ“ Directory Structure</b></summary>

```
wsa/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                    # Flask WebSocket server
â”‚   â”œâ”€â”€ config.py                 # System configuration
â”‚   â”œâ”€â”€ audio_stream/             # Audio preprocessing & CNN-LSTM model
â”‚   â”œâ”€â”€ video_stream/             # Face detection & CNN model
â”‚   â”œâ”€â”€ fusion_engine/            # Multimodal stress fusion
â”‚   â””â”€â”€ utils/                    # Alert & session management
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.js               # Main application
â”‚   â”‚   â”œâ”€â”€ components/          # Dashboard UI components
â”‚   â”‚   â””â”€â”€ services/            # WebSocket & media capture
â”‚   â””â”€â”€ package.json
â””â”€â”€ docs/                         # Documentation
```

</details>

---

## ğŸ“Š Datasets Used

| Modality | Dataset | Purpose |
|----------|---------|---------|
| **Audio** | RAVDESS | Speech emotion recognition training |
| **Audio** | CREMA-D | Speech emotion recognition training |
| **Video** | FER2013 | Facial emotion recognition training |
| **Video** | AffectNet | Facial emotion recognition training |

> **Note:** Datasets used for model training only. Live system does NOT store raw audio/video data.

---

<details>
<summary><b>ğŸ”¬ Feature Extraction Details</b></summary>

### Audio Features
- **MFCC** â€“ 13 coefficients + delta + delta-delta (39 features)
- **Pitch (F0)** â€“ Fundamental frequency using PYIN algorithm
- **Energy** â€“ RMS energy calculation
- **Jitter & Shimmer** â€“ Voice quality indicators
- **Speech Rate** â€“ Syllable detection-based estimation
- **Spectral** â€“ Centroid, rolloff, zero-crossing rate

### Video Features
- **Facial Landmarks** â€“ 468 points via MediaPipe Face Mesh
- **Eye Aspect Ratio (EAR)** â€“ Blink detection and eye openness
- **Eyebrow Height** â€“ Distance from eyebrow to eye
- **Mouth Aspect Ratio (MAR)** â€“ Mouth opening measurement
- **Head Pose** â€“ Pitch, yaw, roll angles
- **Facial Tension** â€“ Symmetry and muscle activation metrics

</details>

---

<details>
<summary><b>ğŸ§  Model Architectures</b></summary>

### Audio Emotion Model (CNN-LSTM)
- **Input:** MFCC time-series (time_frames Ã— 39 features)
- **Architecture:** Conv1D layers â†’ Bidirectional LSTM â†’ Dense layers
- **Output:** 7 emotion probabilities (neutral, happy, sad, angry, fear, disgust, surprise)
- **Framework:** PyTorch

### Video Emotion Model (CNN)
- **Input:** 48Ã—48 grayscale face ROI
- **Architecture:** 4 Conv2D blocks with BatchNorm â†’ Dropout â†’ Dense layers
- **Output:** 7 emotion probabilities
- **Framework:** PyTorch

### Multimodal Fusion
- **Method:** Confidence-weighted averaging
- **Formula:** `stress_final = (w_audio Ã— stress_audio) + (w_video Ã— stress_video)`
- **Weights:** Dynamically adjusted based on model confidence scores

</details>

---

## ğŸ“ˆ Model Performance

| Modality | Model | Accuracy | Latency |
|----------|-------|----------|---------|
| Audio | CNN-LSTM | ~85% | <100ms |
| Video | CNN | ~88% | <50ms |
| **Fused** | **Weighted Avg** | **~91%** | **<150ms** |

---

## ğŸ–¥ï¸ Dashboard Overview

| Tab | Purpose |
|-----|---------|
| **Live Monitor** | Camera feed, audio waveform, current emotions, stress meter |
| **Timeline** | Real-time stress chart (last 100 readings) |
| **Analytics** | Session stats, emotion distribution, peak stress tracking |
| **Alerts** | Active warnings with recommendations |
| **System Info** | Model architecture, privacy policy |

---

<details>
<summary><b>ğŸš€ How to Run</b></summary>

### Prerequisites
- Python 3.8+, Node.js 16+

### Backend Setup
```bash
cd backend
python -m venv venv
venv\Scripts\activate          # Windows
pip install -r requirements.txt
python app.py                  # Runs on port 5000
```

### Frontend Setup
```bash
cd frontend
npm install
npm start                      # Runs on port 3000
```

### Usage
1. Open `http://localhost:3000`
2. Accept privacy consent
3. Grant browser permissions for camera & microphone
4. View live stress analysis

</details>

---

## ğŸ”’ Privacy & Ethics

> **ğŸ›¡ï¸ PRIVACY GUARANTEE**
> 
> âœ… **Zero Raw Data Storage** â€“ Audio/video processed in-memory only  
> âœ… **Session-Based** â€“ All data cleared on disconnect  
> âœ… **No Identity Recognition** â€“ Analyzes emotions, not individuals  
> âœ… **Explicit Consent** â€“ User permission required before any access  
> âœ… **Full Transparency** â€“ Complete model architecture disclosed

**We analyze stress, not identity. Your privacy is non-negotiable.**

---

## ğŸš€ Future Enhancements

- Deep end-to-end multimodal fusion with attention mechanisms
- Transformer-based temporal stress modeling
- Multi-user dashboard for team analytics
- Wearable sensor integration (heart rate, GSR)

---

## ğŸ“ Disclaimer

This system is a **supportive tool** for workplace wellness monitoring. It should **not replace professional medical or psychological assessment**. Results are probabilistic ML estimates.

---

## ğŸ‘¥ Contributors

**Likhitha H S** â€“ Final-Year Computer Science Engineering Project | 2025

---

## ğŸ™ Acknowledgments

MediaPipe (Google) â€¢ LibROSA â€¢ PyTorch Community
